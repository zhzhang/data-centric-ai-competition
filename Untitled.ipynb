{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c42970-8c06-4a74-8189-7ddcdc7b4381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2019 files belonging to 10 classes.\n",
      "Found 813 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Deletes nonsensical examples like smiley faces, hearts, stars, English and Chinese words, unrelated shapes and scribbles.\n",
    "directory = \"cleaned_nonsensical_examples\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 8\n",
    "tf.random.set_seed(123)\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory + '/train',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(32, 32),\n",
    ")\n",
    "\n",
    "valid = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory + '/val',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(32, 32),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0570be34-750b-4092-8c66-13ff55899d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_4 ( (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "tf.nn.bias_add_4 (TFOpLambda (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "model_6 (Functional)         (None, 8, 8, 256)         229760    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 232,330\n",
      "Trainable params: 229,386\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n",
      "102/102 [==============================] - 1s 4ms/step - loss: 33.4250 - accuracy: 0.0996\n",
      "loss 33.425010681152344, acc 0.09963099658489227\n",
      "Epoch 1/100\n",
      "253/253 [==============================] - 4s 14ms/step - loss: 2.0074 - accuracy: 0.3076 - val_loss: 1.9503 - val_accuracy: 0.2706\n",
      "Epoch 2/100\n",
      "253/253 [==============================] - 3s 13ms/step - loss: 1.4729 - accuracy: 0.5176 - val_loss: 1.6586 - val_accuracy: 0.4096\n",
      "Epoch 3/100\n",
      "250/253 [============================>.] - ETA: 0s - loss: 1.1968 - accuracy: 0.6315"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nz/80pd7lrj2839ywjlfh0c65s4qx02sm/T/ipykernel_91689/2908689045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1214\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/data-centric-ai-competition-lQUmyd6X-py3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "\n",
    "directory = \"cleaned_nonsensical_examples\"\n",
    "user_data = directory + \"/train\"\n",
    "valid_data = directory + \"/val\"\n",
    "test_data = directory + \"/test\" # this can be the label book, or any other test set you create\n",
    "\n",
    "### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###\n",
    "batch_size = 8\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "total_length = ((train.cardinality() + valid.cardinality()) * batch_size).numpy()\n",
    "if total_length > 10_000:\n",
    "    print(f\"Dataset size larger than 10,000. Got {total_length} examples\")\n",
    "    sys.exit()\n",
    "\n",
    "# test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     test_data,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"categorical\",\n",
    "#     class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(32, 32, 3),\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    ")\n",
    "base_model = tf.keras.Model(\n",
    "    base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\n",
    ")\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(10)(x)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "loss_0, acc_0 = model.evaluate(valid)\n",
    "print(f\"loss {loss_0}, acc {acc_0}\")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "loss, acc = model.evaluate(valid)\n",
    "print(f\"final loss {loss}, final acc {acc}\")\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(test)\n",
    "# print(f\"test loss {test_loss}, test acc {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "353b8ed9-a1f8-4f97-8c57-a71812ae6165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 264)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "for f in glob.glob('base_data/train/**', recursive=True):\n",
    "    if \".png\" not in f:\n",
    "        continue\n",
    "    img = mpimg.imread(f)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show(block=True)\n",
    "    print(img.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069ae71-903f-4460-82ca-b514ad3f9c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
