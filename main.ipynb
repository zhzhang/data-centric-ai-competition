{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7c42970-8c06-4a74-8189-7ddcdc7b4381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1936 files belonging to 10 classes.\n",
      "Found 4927 files belonging to 10 classes.\n",
      "Using 493 files for training.\n",
      "Found 827 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Deletes nonsensical examples like smiley faces, hearts, stars, English and Chinese words, unrelated shapes and scribbles.\n",
    "# directory = \"cleaned_nonsensical_examples\"\n",
    "\n",
    "# directory = \"base_data\"\n",
    "directory = \"cleaned_manual_relabeled\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "img_size = 32\n",
    "\n",
    "class_names = [\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"]\n",
    "batch_size = 8\n",
    "tf.random.set_seed(123)\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory + '/train',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_size, img_size),\n",
    ")\n",
    "\n",
    "font_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"font_based\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.90,\n",
    "    subset=\"training\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_size, img_size),\n",
    ")\n",
    "\n",
    "valid = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory + '/val',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_size, img_size),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84471af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "@tf.function\n",
    "def augment(images, labels):\n",
    "    def map_fn(im):\n",
    "        white = tf.constant(255)\n",
    "        im = tfa.image.shear_x(im, tf.random.uniform(shape=[], minval=-0.4,maxval=0.4), white)\n",
    "        # im = tfa.image.translate(im, tf.random.uniform(shape=[2], minval=-3, maxval=3), fill_mode=\"nearest\")\n",
    "        return im\n",
    "    return tf.map_fn(map_fn, images), labels\n",
    "\n",
    "train_shear = train.map(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570be34-750b-4092-8c66-13ff55899d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_12  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "tf.nn.bias_add_12 (TFOpLambd (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "model_24 (Functional)        (None, 8, 8, 256)         229760    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_12  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 232,330\n",
      "Trainable params: 229,386\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n",
      "104/104 [==============================] - 0s 2ms/step - loss: 33.3683 - accuracy: 0.0992\n",
      "loss 33.36829376220703, acc 0.09915357083082199\n",
      "Epoch 1/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 1.8046 - accuracy: 0.3769 - val_loss: 1.7126 - val_accuracy: 0.4039\n",
      "Epoch 2/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 1.2782 - accuracy: 0.5771 - val_loss: 1.1279 - val_accuracy: 0.6058\n",
      "Epoch 3/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 1.0227 - accuracy: 0.6607 - val_loss: 0.9874 - val_accuracy: 0.6288\n",
      "Epoch 4/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.8964 - accuracy: 0.7063 - val_loss: 1.3107 - val_accuracy: 0.5248\n",
      "Epoch 5/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.7794 - accuracy: 0.7512 - val_loss: 0.9758 - val_accuracy: 0.6590\n",
      "Epoch 6/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.7181 - accuracy: 0.7668 - val_loss: 0.9979 - val_accuracy: 0.6530\n",
      "Epoch 7/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.6597 - accuracy: 0.7908 - val_loss: 0.9988 - val_accuracy: 0.6542\n",
      "Epoch 8/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.6111 - accuracy: 0.8062 - val_loss: 0.8837 - val_accuracy: 0.7098\n",
      "Epoch 9/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.5561 - accuracy: 0.8279 - val_loss: 1.3150 - val_accuracy: 0.6058\n",
      "Epoch 10/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.5308 - accuracy: 0.8337 - val_loss: 1.3120 - val_accuracy: 0.5937\n",
      "Epoch 11/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.4957 - accuracy: 0.8536 - val_loss: 1.1314 - val_accuracy: 0.6227\n",
      "Epoch 12/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.4637 - accuracy: 0.8557 - val_loss: 0.5868 - val_accuracy: 0.8041\n",
      "Epoch 13/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.4403 - accuracy: 0.8614 - val_loss: 0.7257 - val_accuracy: 0.7618\n",
      "Epoch 14/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3988 - accuracy: 0.8763 - val_loss: 0.8411 - val_accuracy: 0.7195\n",
      "Epoch 15/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3759 - accuracy: 0.8889 - val_loss: 0.6766 - val_accuracy: 0.7775\n",
      "Epoch 16/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3768 - accuracy: 0.8866 - val_loss: 0.5636 - val_accuracy: 0.8235\n",
      "Epoch 17/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3473 - accuracy: 0.8923 - val_loss: 0.5614 - val_accuracy: 0.8162\n",
      "Epoch 18/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3380 - accuracy: 0.9010 - val_loss: 0.9425 - val_accuracy: 0.7098\n",
      "Epoch 19/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3183 - accuracy: 0.9024 - val_loss: 0.8831 - val_accuracy: 0.7110\n",
      "Epoch 20/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.3088 - accuracy: 0.9074 - val_loss: 1.1537 - val_accuracy: 0.6530\n",
      "Epoch 21/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2861 - accuracy: 0.9134 - val_loss: 0.6644 - val_accuracy: 0.8017\n",
      "Epoch 22/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2862 - accuracy: 0.9090 - val_loss: 0.7056 - val_accuracy: 0.7739\n",
      "Epoch 23/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2726 - accuracy: 0.9157 - val_loss: 0.5663 - val_accuracy: 0.8295\n",
      "Epoch 24/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2709 - accuracy: 0.9180 - val_loss: 0.5816 - val_accuracy: 0.8319\n",
      "Epoch 25/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2409 - accuracy: 0.9269 - val_loss: 0.5624 - val_accuracy: 0.8380\n",
      "Epoch 26/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2145 - accuracy: 0.9356 - val_loss: 0.6770 - val_accuracy: 0.8247\n",
      "Epoch 27/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2346 - accuracy: 0.9290 - val_loss: 0.7125 - val_accuracy: 0.7969\n",
      "Epoch 28/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2266 - accuracy: 0.9304 - val_loss: 0.5141 - val_accuracy: 0.8428\n",
      "Epoch 29/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2111 - accuracy: 0.9329 - val_loss: 0.6535 - val_accuracy: 0.8029\n",
      "Epoch 30/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.2011 - accuracy: 0.9381 - val_loss: 0.5721 - val_accuracy: 0.8247\n",
      "Epoch 31/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1982 - accuracy: 0.9379 - val_loss: 0.7168 - val_accuracy: 0.8126\n",
      "Epoch 32/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1879 - accuracy: 0.9407 - val_loss: 0.6274 - val_accuracy: 0.8259\n",
      "Epoch 33/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1982 - accuracy: 0.9411 - val_loss: 0.6550 - val_accuracy: 0.8114\n",
      "Epoch 34/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1747 - accuracy: 0.9448 - val_loss: 0.7244 - val_accuracy: 0.8077\n",
      "Epoch 35/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1821 - accuracy: 0.9404 - val_loss: 0.5807 - val_accuracy: 0.8235\n",
      "Epoch 36/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1768 - accuracy: 0.9455 - val_loss: 0.7120 - val_accuracy: 0.8077\n",
      "Epoch 37/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1666 - accuracy: 0.9471 - val_loss: 0.7380 - val_accuracy: 0.8114\n",
      "Epoch 38/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1629 - accuracy: 0.9507 - val_loss: 0.6945 - val_accuracy: 0.8150\n",
      "Epoch 39/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1528 - accuracy: 0.9512 - val_loss: 0.6429 - val_accuracy: 0.8235\n",
      "Epoch 40/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1496 - accuracy: 0.9528 - val_loss: 0.6738 - val_accuracy: 0.8065\n",
      "Epoch 41/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1555 - accuracy: 0.9535 - val_loss: 0.6179 - val_accuracy: 0.8295\n",
      "Epoch 42/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1444 - accuracy: 0.9604 - val_loss: 0.7518 - val_accuracy: 0.7908\n",
      "Epoch 43/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1431 - accuracy: 0.9562 - val_loss: 0.8977 - val_accuracy: 0.7763\n",
      "Epoch 44/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1568 - accuracy: 0.9510 - val_loss: 0.6326 - val_accuracy: 0.8271\n",
      "Epoch 45/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1485 - accuracy: 0.9528 - val_loss: 0.6510 - val_accuracy: 0.8271\n",
      "Epoch 46/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1328 - accuracy: 0.9592 - val_loss: 0.6792 - val_accuracy: 0.8150\n",
      "Epoch 47/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1302 - accuracy: 0.9588 - val_loss: 0.5982 - val_accuracy: 0.8307\n",
      "Epoch 48/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1270 - accuracy: 0.9631 - val_loss: 0.7632 - val_accuracy: 0.7908\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1259 - accuracy: 0.9585 - val_loss: 0.7322 - val_accuracy: 0.8138\n",
      "Epoch 50/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1229 - accuracy: 0.9643 - val_loss: 0.6627 - val_accuracy: 0.8222\n",
      "Epoch 51/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1284 - accuracy: 0.9627 - val_loss: 0.7768 - val_accuracy: 0.8162\n",
      "Epoch 52/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1072 - accuracy: 0.9668 - val_loss: 0.6872 - val_accuracy: 0.8259\n",
      "Epoch 53/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1321 - accuracy: 0.9583 - val_loss: 0.6759 - val_accuracy: 0.8198\n",
      "Epoch 54/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1235 - accuracy: 0.9622 - val_loss: 0.8150 - val_accuracy: 0.8017\n",
      "Epoch 55/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1203 - accuracy: 0.9647 - val_loss: 0.7785 - val_accuracy: 0.8053\n",
      "Epoch 56/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0980 - accuracy: 0.9677 - val_loss: 0.7048 - val_accuracy: 0.8126\n",
      "Epoch 57/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1075 - accuracy: 0.9645 - val_loss: 0.6414 - val_accuracy: 0.8440\n",
      "Epoch 58/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1123 - accuracy: 0.9666 - val_loss: 0.7743 - val_accuracy: 0.8150\n",
      "Epoch 59/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1030 - accuracy: 0.9688 - val_loss: 0.8154 - val_accuracy: 0.7944\n",
      "Epoch 60/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1005 - accuracy: 0.9677 - val_loss: 0.8534 - val_accuracy: 0.7848\n",
      "Epoch 61/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0975 - accuracy: 0.9723 - val_loss: 0.6434 - val_accuracy: 0.8440\n",
      "Epoch 62/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1010 - accuracy: 0.9693 - val_loss: 0.7907 - val_accuracy: 0.8222\n",
      "Epoch 63/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1103 - accuracy: 0.9668 - val_loss: 0.6389 - val_accuracy: 0.8380\n",
      "Epoch 64/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0886 - accuracy: 0.9746 - val_loss: 0.6925 - val_accuracy: 0.8259\n",
      "Epoch 65/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0872 - accuracy: 0.9732 - val_loss: 0.7658 - val_accuracy: 0.8102\n",
      "Epoch 66/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0917 - accuracy: 0.9730 - val_loss: 0.7201 - val_accuracy: 0.8247\n",
      "Epoch 67/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1052 - accuracy: 0.9668 - val_loss: 0.7211 - val_accuracy: 0.8331\n",
      "Epoch 68/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.1075 - accuracy: 0.9700 - val_loss: 0.7578 - val_accuracy: 0.8222\n",
      "Epoch 69/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0866 - accuracy: 0.9714 - val_loss: 0.7253 - val_accuracy: 0.8259\n",
      "Epoch 70/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0773 - accuracy: 0.9766 - val_loss: 0.7342 - val_accuracy: 0.8210\n",
      "Epoch 71/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0941 - accuracy: 0.9714 - val_loss: 0.8702 - val_accuracy: 0.7836\n",
      "Epoch 72/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0795 - accuracy: 0.9743 - val_loss: 0.7571 - val_accuracy: 0.8210\n",
      "Epoch 73/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0734 - accuracy: 0.9753 - val_loss: 0.7203 - val_accuracy: 0.8259\n",
      "Epoch 74/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0780 - accuracy: 0.9782 - val_loss: 0.7061 - val_accuracy: 0.8186\n",
      "Epoch 75/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0911 - accuracy: 0.9734 - val_loss: 0.7731 - val_accuracy: 0.8222\n",
      "Epoch 76/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0796 - accuracy: 0.9766 - val_loss: 0.6724 - val_accuracy: 0.8138\n",
      "Epoch 77/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0831 - accuracy: 0.9716 - val_loss: 0.6276 - val_accuracy: 0.8319\n",
      "Epoch 78/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0859 - accuracy: 0.9734 - val_loss: 0.8257 - val_accuracy: 0.8041\n",
      "Epoch 79/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0749 - accuracy: 0.9748 - val_loss: 0.7784 - val_accuracy: 0.8077\n",
      "Epoch 80/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0792 - accuracy: 0.9746 - val_loss: 0.8181 - val_accuracy: 0.8029\n",
      "Epoch 81/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0825 - accuracy: 0.9725 - val_loss: 0.7287 - val_accuracy: 0.8319\n",
      "Epoch 82/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0668 - accuracy: 0.9771 - val_loss: 0.7249 - val_accuracy: 0.8271\n",
      "Epoch 83/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0706 - accuracy: 0.9764 - val_loss: 0.7430 - val_accuracy: 0.8222\n",
      "Epoch 84/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0810 - accuracy: 0.9737 - val_loss: 0.7248 - val_accuracy: 0.8235\n",
      "Epoch 85/100\n",
      "546/546 [==============================] - 3s 5ms/step - loss: 0.0694 - accuracy: 0.9812 - val_loss: 0.7385 - val_accuracy: 0.8356\n",
      "Epoch 86/100\n",
      "106/546 [====>.........................] - ETA: 1s - loss: 0.0137 - accuracy: 0.9976"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "\n",
    "directory = \"cleaned_nonsensical_examples\"\n",
    "user_data = directory + \"/train\"\n",
    "valid_data = directory + \"/val\"\n",
    "test_data = directory + \"/test\" # this can be the label book, or any other test set you create\n",
    "\n",
    "### DO NOT MODIFY BELOW THIS LINE, THIS IS THE FIXED MODEL ###\n",
    "batch_size = 8\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "total_length = ((train.cardinality() + valid.cardinality()) * batch_size).numpy()\n",
    "if total_length > 10_000:\n",
    "    print(f\"Dataset size larger than 10,000. Got {total_length} examples\")\n",
    "    sys.exit()\n",
    "\n",
    "# test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     test_data,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"categorical\",\n",
    "#     class_names=[\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\"],\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(32, 32, 3),\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    ")\n",
    "base_model = tf.keras.Model(\n",
    "    base_model.inputs, outputs=[base_model.get_layer(\"conv2_block3_out\").output]\n",
    ")\n",
    "\n",
    "inputs = tf.keras.Input(shape=(img_size, img_size, 3))\n",
    "### Random Zoom\n",
    "# x = tf.keras.layers.RandomZoom([0.1, 0.1])(inputs)\n",
    "# x = tf.keras.layers.RandomRotation([0.05, 0.05])(x)\n",
    "\n",
    "###\n",
    "x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(10)(x)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "loss_0, acc_0 = model.evaluate(valid)\n",
    "print(f\"loss {loss_0}, acc {acc_0}\")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    # train.concatenate(train_shear),\n",
    "    train.concatenate(train_shear).concatenate(font_train),\n",
    "    # font_train,\n",
    "    validation_data=valid,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "loss, acc = model.evaluate(valid)\n",
    "print(f\"final loss {loss}, final acc {acc}\")\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(test)\n",
    "# print(f\"test loss {test_loss}, test acc {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074c0c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "353b8ed9-a1f8-4f97-8c57-a71812ae6165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(451, 281)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "for f in glob.glob('base_data/train/**', recursive=True):\n",
    "    if \".png\" not in f:\n",
    "        continue\n",
    "    img = mpimg.imread(f)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show(block=True)\n",
    "    print(img.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c5474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFont, ImageDraw\n",
    "import os\n",
    "import uuid\n",
    "from random import randint\n",
    "\n",
    "os.mkdir(\"font_based\")\n",
    "for c in class_names:\n",
    "    os.mkdir(os.path.join(\"font_based\", c))\n",
    "\n",
    "def _generate(font, c, upper):\n",
    "    image = Image.new(\"RGBA\", (128,128), (255,255,255))\n",
    "    usr_font = ImageFont.truetype(font, randint(40, 80))\n",
    "    d_usr = ImageDraw.Draw(image)\n",
    "    d_usr.fontmode = \"1\" # this apparently sets (anti)aliasing.  See link below.\n",
    "    d_usr.text((randint(10, 40), randint(10, 40)), c.upper() if upper else c,(0,0,0), font=usr_font)\n",
    "    image.save(os.path.join(\"font_based\", c, f\"{uuid.uuid1()}.png\"))\n",
    "\n",
    "def generate(font):\n",
    "    for c in class_names:\n",
    "        _generate(font, c, False)\n",
    "        _generate(font, c, True)\n",
    "\n",
    "        \n",
    "font_root = \"../fonts-main/ofl\"\n",
    "for font in os.listdir(font_root):\n",
    "    if randint(1, 10) > 1:\n",
    "        continue\n",
    "    for f in os.listdir(font_root + \"/\" + font):\n",
    "        if f.endswith(\".ttf\"):\n",
    "            generate(os.path.join(font_root, font, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34180fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b0945b92-0638-11ec-b450-9b9d4b3c6bbe'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "str(uuid.uuid1())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff7af7",
   "metadata": {},
   "source": [
    "Base Data Validation Accuracy 0.6777\n",
    "Cleaned nonsensical final loss 0.7061\n",
    "\n",
    "+ Random Zoom 0.1, 0.6747\n",
    "+ Random Shear 0.4 0.8198\n",
    "\n",
    "Generated data only 0.5949\n",
    "\n",
    "Shear + Generated 0.8464\n",
    " + Half generated 0.8548\n",
    " + quarter generated 0.8694\n",
    "\n",
    "Train Found 1936 files belonging to 10 classes.\n",
    "Font Found 4927 files belonging to 10 classes.\n",
    "Valid Found 827 files belonging to 10 classes.\n",
    "\n",
    "\n",
    "Validation data has no mis-scans that are offset by half the image.\n",
    "- Allows extraneous lines on the page.\n",
    "- Allows some hearts and smileys.\n",
    "\n",
    "Flipping, cropping, random erasure are not viable techniques for MNIST because of the geometry of the characters.\n",
    "However, shear, light rotation, and translation ARE viable techniques.\n",
    "\n",
    "Random shears between -0.4 and 0.4 final loss 0.5460479259490967, final acc 0.8440145254135132\n",
    "Shears + zoom final loss 0.6536434292793274, final acc 0.8536880016326904\n",
    "0.823 w/ generated data.\n",
    "final loss 0.9184684157371521, final acc 0.8162031173706055 with 0.25\n",
    "\n",
    "\n",
    "GAN https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c32fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
